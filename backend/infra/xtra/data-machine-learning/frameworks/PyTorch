https://github.com/googlecloudplatform/dataflux-pytorch # Increase ML-training speed by up to 3x when using data stored in GCS for training datasets by using the dataflux accelerated dataloader
https://github.com/googlecloudplatform/ml-testing-accelerators # Testing framework for Deep Learning models (Tensorflow and PyTorch) on Google Cloud hardware accelerators (TPU and GPU)
https://github.com/googlecloudplatform/dataflux-client-python # This is the git repository for the Dataflux Python client library, providing fast listing and download of small files from GCS in Python.
https://github.com/google/gemma_pytorch # The official PyTorch implementation of Google's Gemma models
https://github.com/google/jetstream-pytorch # PyTorch/XLA integration with JetStream for LLM inference
https://github.com/google/jetstream # JetStream is a throughput and memory optimized engine for LLM inference on XLA devices, starting with TPUs (and GPUs in future -- PRs welcome).

https://github.com/google-deepmind/tapnet # Tracking Any Point (TAP)
https://github.com/google-deepmind/recurrentgemma # Open weights language model from Google DeepMind, based on Griffin.
https://github.com/google-deepmind/graph_nets # Build Graph Nets in Tensorflow
https://github.com/google-deepmind/dks # Modify neural network models (and their initializations) to make them easier to train.
https://github.com/google-deepmind/treescope # An interactive HTML pretty-printer for machine learning research in IPython notebooks.
https://github.com/google-deepmind/dm_nevis # NEVIS'22: Benchmarking the next generation of never-ending learners
https://github.com/google-deepmind/jraph # A Graph Neural Network Library in Jax
https://github.com/google-deepmind/geomatch #
https://github.com/google-deepmind/asyncdiloco #
https://github.com/google-deepmind/threednel #
https://github.com/google-deepmind/c3_neural_compression #

https://github.com/azure-samples/functions-python-pytorch-tutorial # Use Python, PyTorch, and Azure Functions to classify an image
https://github.com/azure-samples/azure-functions-pytorch-image-identify # End to end example of an image identification app running on Azure Functions and showing capabilities like Remote Build and Azure Files support
https://github.com/azure-samples/functions-deploy-pytorch-onnx # Sample demonstrating deployment of Pytorch models through ONNX within Azure Functions

https://github.com/azure/ms-amp # Microsoft Automatic Mixed Precision Library
https://github.com/azure/optimized-pytorch-on-databricks-and-fabric # Train Hugging Face models with PyTorch FSDP on Azure Databricks and optimized inference with BetterTransformer on Microsoft Fabric
https://github.com/azure/nlp-samples # Japanese NLP sample codes
https://github.com/azure/machine-learning-at-scale # machine learning at scale on Azure Machine Learning
https://github.com/azure/azure-storage-for-pytorch # Azure Storage integrations for PyTorch. This project is a work-in-progress.

https://github.com/microsoft/bringing-old-photos-back-to-life # Bringing Old Photo Back to Life (CVPR 2020 oral)
https://github.com/microsoft/lora # Code for loralib, an implementation of "LoRA: Low-Rank Adaptation of Large Language Models"

https://github.com/aws-solutions-library-samples/guidance-for-machine-learning-inference-on-aws # Deploy a machine learning inference architecture on Amazon Elastic Kubernetes Service (Amazon EKS).

https://github.com/aws-samples/amazon-sagemaker-bert-pytorch #
https://github.com/aws-samples/amazon-sagemaker-pytorch-detectron2 # This repository shows how to train an object detection algorithm with Detectron2 on Amazon SageMaker
https://github.com/aws-samples/amazon-sagemaker-bert-classify-pytorch # This sample show you how to train BERT on Amazon Sagemaker using Spot instances
https://github.com/aws-samples/amazon-sagemaker-audio-classification-pytorch #
https://github.com/aws-samples/aws-lambda-docker-serverless-inference # Serve scikit-learn, XGBoost, TensorFlow, and PyTorch models with AWS Lambda container images support.
https://github.com/aws-samples/amazon-sagemaker-mask-r-cnn-pytorch #
https://github.com/aws-samples/amazon-sagemaker-endpoint-deployment-of-fastai-model-with-torchserve # Deploy FastAI Trained PyTorch Model in TorchServe and Host in Amazon SageMaker Inference Endpoint
https://github.com/aws-samples/sagemaker-benchmarking-accelerators-pretrained-pytorch-resnet50 #
https://github.com/aws-samples/sagemaker-distributed-training-pytorch-kr # Hands-on lab that applies Sagemaker Distributed Training to image classification task. Written in Korean for Korean customers.
https://github.com/aws-samples/torchserve-eks # How to deploy TorchServe on an Amazon EKS cluster for inference.
https://github.com/aws-samples/sagemaker-a2d2-segmentation-pytorch #
https://github.com/aws-samples/aws-lambda-container-inference-pytorch #
https://github.com/aws-samples/amazon-sagemaker-endpoint-deployment-of-siamese-network-with-torchserve # Twin Neural Network Training with PyTorch and fast.ai and its Deployment with TorchServe on Amazon SageMaker
https://github.com/aws-samples/amazon-sagemaker-tsp-deep-rl # train, deploy, and make inferences using deep reinforcement learning to solve the Travelling Salesperson Problem
https://github.com/aws-samples/deploy-stable-diffusion-model-on-amazon-sagemaker-endpoint # Deploy Stable Diffusion Model on Amazon SageMaker Endpont
https://github.com/aws-samples/amazon-sagemaker-visual-transformer # Image Classification using Visual Transformers in Amazon SageMaker based on the ideas from a research paper
https://github.com/aws-samples/amazon-sagemaker-local-mode # Amazon SageMaker Local Mode Examples
https://github.com/aws-samples/amazon-sagemaker-pipelines-serial-batch-inference-sklearn-pytorch # Bring-your-own-feature and bring your-own-model serial inferencing examples for batch inference MLOps with SageMaker Pipelines.
https://github.com/aws-samples/aws-do-kubeflow # A do-framework project to simplify deployment of Kubeflow on AWS
https://github.com/aws-samples/amazon-sagemaker-managed-spot-training # Amazon SageMaker Managed Spot Training Examples
https://github.com/aws-samples/sagemaker-cv-preprocessing-training-performance # SageMaker training implementation for computer vision to offload JPEG decoding and augmentations on GPUs using Nvidia Dali
https://github.com/aws-samples/aws-do-pm # A framework for building predictive modeling applications
https://github.com/aws-samples/awsome-fmops # Collection of best practices, reference architectures, examples, and utilities for foundation model development and deployment on AWS.

https://github.com/aws/sagemaker-pytorch-training-toolkit # Toolkit for running PyTorch training scripts on SageMaker.
https://github.com/aws/sagemaker-pytorch-inference-toolkit # Toolkit for allowing inference and serving with PyTorch on SageMaker. Dockerfiles used for building SageMaker Pytorch Containers are at
https://github.com/aws/deep-learning-containers.
https://github.com/aws/amazon-s3-plugin-for-pytorch #
https://github.com/aws/sagemaker-python-sdk # A library for training and deploying machine learning models on Amazon SageMaker
https://github.com/aws/deep-learning-containers # AWS Deep Learning Containers (DLCs) are a set of Docker images for training and serving models in TensorFlow, TensorFlow 2, PyTorch, and MXNet.
https://github.com/aws/sagemaker-distribution # A set of Docker images that include popular frameworks for machine learning, data science and visualization.

https://github.com/oracle-samples/oci-data-science-ai-samples # Tutorials and code examples highlighting different features of the OCI Data Science and AI services

https://github.com/nvidia/flownet2-pytorch # Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks
https://github.com/nvidia/apex # A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch
https://github.com/nvidia/tacotron2 # Tacotron 2 - PyTorch implementation with faster-than-realtime inference
https://github.com/nvidia/fastertransformer # Transformer related optimization, including BERT, GPT
https://github.com/nvidia/vid2vid # Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.
https://github.com/nvidia/bigvgan # Official PyTorch implementation of BigVGAN (ICLR 2023)
https://github.com/nvidia/cleanunet # Official PyTorch Implementation of CleanUNet (ICASSP 2022)
https://github.com/nvidia/pyprof # A GPU performance profiling tool for PyTorch models
https://github.com/nvidia/tao_pytorch_backend # TAO Toolkit deep learning networks with PyTorch backend
https://github.com/nvidia/torch-harmonics # Differentiable signal processing on the sphere for PyTorch
https://github.com/nvidia/modulus # Framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods
https://github.com/nvidia/transformer-ls # Official PyTorch Implementation of Long-Short Transformer (NeurIPS 2021).
https://github.com/nvidia/pix2pixhd # Synthesizing and manipulating 2048x1024 images with conditional GANs
https://github.com/nvidia/retinanet-examples # Fast and accurate object detection with end-to-end GPU optimization
https://github.com/nvidia/transformerengine # A library for accelerating training and inference using 8-bit floating point (FP8) precision on Hopper and Ada GPUs
https://github.com/nvidia/modulus-sym # Framework providing pythonic APIs, algorithms and utilities to be used with Modulus
https://github.com/nvidia/deeplearningexamples # State-of-the-art Deep Learning scripts organized by models - easy to train and deploy
https://github.com/nvidia/torchfort # An Online Deep Learning Interface for HPC programs on NVIDIA GPUs
https://github.com/nvidia/modulus-launch # Repo of optimized training recipes for accelerating PyTorch workflows of AI driven surrogates for physical systems
https://github.com/nvidia/dali # A GPU-accelerated library for deep learning training and inference applications.
https://github.com/nvidia/audio-flamingo # PyTorch implementation of Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities.
https://github.com/nvidia/clara-train-examples # Example notebooks demonstrating how to use Clara Train to build Medical Imaging Deep Learning models
https://github.com/nvidia/minkowskiengine # Minkowski Engine is an auto-diff neural network library for high-dimensional sparse tensors
https://github.com/nvidia/framework-reproducibility # Providing reproducibility in deep learning frameworks
https://github.com/nvidia/nvimagecodec # A nvImageCodec library of GPU- and CPU- accelerated codecs featuring a unified interface

https://github.com/lightning-ai/pytorch-lightning # Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.
https://github.com/lightning-ai/litgpt # 20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.
https://github.com/lightning-ai/lit-llama # Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training.
https://github.com/lightning-ai/litserve # Lightning-fast serving engine for AI models. Flexible. Easy. Enterprise-scale.
https://github.com/lightning-ai/torchmetrics # Torchmetrics - Machine learning metrics for distributed, scalable PyTorch applications.
https://github.com/lightning-ai/deep-learning-project-template # Pytorch Lightning code guideline for conferences
https://github.com/lightning-ai/lightning-thunder # Make PyTorch models up to 40% faster! Thunder is a source to source compiler for PyTorch. It enables using different hardware executors at once; across one or thousands of GPUs.
https://github.com/lightning-ai/dl-fundamentals # Deep Learning Fundamentals -- Code material and exercises
https://github.com/lightning-ai/litdata # Transform datasets at scale. Optimize datasets for fast AI model training.
https://github.com/lightning-ai/tutorials # Collection of Pytorch lightning tutorial form as rich scripts automatically transformed to ipython notebooks.
https://github.com/lightning-ai/engineering-class # Lightning Bits: Engineering for Researchers repo
https://github.com/lightning-ai/utilities # Common Python utilities and GitHub Actions in Lightning Ecosystem
https://github.com/lightning-ai/ecosystem-ci # Automate issue discovery for your projects against Lightning nightly and releases.
https://github.com/lightning-ai/lightning-habana # Lightning support for Intel Habana accelerators.
https://github.com/lightning-ai/lightning-multinode-templates # Multinode templates for Pytorch Lightning
https://github.com/lightning-ai/cloud-training-workshop #
https://github.com/lightning-ai/.github #

https://github.com/pytorch-labs/gpt-fast # Simple and efficient pytorch-native transformer text generation in <1000 LOC of python.
https://github.com/pytorch-labs/segment-anything-fast # A batched offline inference oriented version of segment-anything
https://github.com/pytorch-labs/attention-gym # Helpful tools and examples for working with flex-attention
https://github.com/pytorch-labs/leanrl # LeanRL is a fork of CleanRL, where selected PyTorch scripts optimized for performance using compile and cudagraphs.
https://github.com/pytorch-labs/applied-ai # Applied AI experiments and examples for PyTorch
https://github.com/pytorch-labs/torchfix # TorchFix - a linter for PyTorch-using code with autofix support
https://github.com/pytorch-labs/triton-cpu # An experimental CPU backend for Triton (https//github.com/openai/triton)
https://github.com/pytorch-labs/facto # Framework for Algorithmic Correctness Testing of Operators

https://github.com/pytorch-ignite/code-generator # Web Application to generate your training scripts with PyTorch Ignite
https://github.com/pytorch-ignite/pytorch-ignite.ai # PyTorch-Ignite website
https://github.com/pytorch-ignite/examples # Examples, tutorials, and how-to guides
https://github.com/pytorch-ignite/idist-snippets #
https://github.com/pytorch-ignite/aws-parallel-cluster-slurm # AWS Parallel Cluster (SLURM) configuration and information
https://github.com/pytorch-ignite/playground # PyTorch-Ignite playground with Binder and Colab
https://github.com/pytorch-ignite/download-mnist-github-action # Github Action to download MNIST
https://github.com/pytorch-ignite/pydata-riyadh-2022-slides # PyData Riyadh 2022 mentored sprint slides
https://github.com/pytorch-ignite/pydata-global2021-slides # Slides for mentored sprint at PyData Global 2021
https://github.com/pytorch-ignite/nbs #

https://github.com/pytorch/pytorch # Tensors and Dynamic neural networks in Python with strong GPU acceleration
https://github.com/pytorch/examples # A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.
https://github.com/pytorch/vision # Datasets, Transforms and Models specific to Computer Vision
https://github.com/pytorch/tutorials # PyTorch tutorials.
https://github.com/pytorch/captum # Model interpretability and understanding for PyTorch
https://github.com/pytorch/ignite # High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.
https://github.com/pytorch/serve # Serve, optimize and scale PyTorch models in production
https://github.com/pytorch/torchtune # A Native-PyTorch Library for LLM Fine-tuning
https://github.com/pytorch/text # Models, data loaders and abstractions for language processing, powered by PyTorch
https://github.com/pytorch/glow # Compiler for Neural Network hardware accelerators
https://github.com/pytorch/torchchat # Run PyTorch LLMs locally on servers, desktop and mobile
https://github.com/pytorch/botorch # Bayesian optimization in PyTorch
https://github.com/pytorch/tensorrt # PyTorch/TorchScript/FX compiler for NVIDIA GPUs using TensorRT
https://github.com/pytorch/audio # Data manipulation and transformation for audio signal processing, powered by PyTorch
https://github.com/pytorch/xla # Enabling PyTorch on XLA Devices (e.g. Google TPU)
https://github.com/pytorch/torchtitan # A native PyTorch Library for large model training
https://github.com/pytorch/rl # A modular, primitive-first, python-first PyTorch library for Reinforcement Learning.
https://github.com/pytorch/torchrec # Pytorch domain library for recommendation systems
https://github.com/pytorch/executorch # On-device AI across mobile, embedded and edge for PyTorch
https://github.com/pytorch/opacus # Training PyTorch models with differential privacy
https://github.com/pytorch/tnt # A lightweight library for PyTorch training tools and utilities
https://github.com/pytorch/functorch # functorch is JAX-like composable function transforms for PyTorch.
https://github.com/pytorch/hub # Submission to https://pytorch.org/hub/
https://github.com/pytorch/fbgemm # FB (Facebook) + GEMM (General Matrix-Matrix Multiplication) - https://code.fb.com/ml-applications/fbgemm/
https://github.com/pytorch/data # A PyTorch repo for data loading and utilities to be shared by the PyTorch domain libraries.
https://github.com/pytorch/torchdynamo # A Python-level JIT compiler designed to make unmodified PyTorch programs faster.
https://github.com/pytorch/cpuinfo # CPU INFOrmation library (x86/x86-64/ARM/ARM64, Linux/Windows/Android/macOS/iOS)
https://github.com/pytorch/extension-cpp # C++ extensions in PyTorch
https://github.com/pytorch/benchmark # TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.
https://github.com/pytorch/ao # PyTorch native quantization and sparsity for training and inference
https://github.com/pytorch/tensordict # TensorDict is a pytorch dedicated tensor container.
https://github.com/pytorch/pippy # Pipeline Parallelism for PyTorch
https://github.com/pytorch/kineto # A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters.
https://github.com/pytorch/torcharrow # High performance model preprocessing library on PyTorch
https://github.com/pytorch/ort # Accelerate PyTorch models with ONNX Runtime
https://github.com/pytorch/torchx # TorchX is a universal job launcher for PyTorch applications. TorchX is designed to have fast iteration time for training/research and support for E2E production ML pipelines when you're ready.
https://github.com/pytorch/builder # Continuous builder and binary build scripts for pytorch
https://github.com/pytorch/pytorch.github.io # The website for PyTorch
https://github.com/pytorch/torcheval # A library that contains a rich collection of performant PyTorch model metrics
https://github.com/pytorch/cppdocs # PyTorch C++ API Documentation
https://github.com/pytorch/workshops # This is a repository for all workshop related materials.
https://github.com/pytorch/hydra-torch # Configuration classes enabling type-safe PyTorch configuration for Hydra apps
https://github.com/pytorch/multipy # torch::deploy (multipy for non-torch uses) is a system that lets you get around the GIL problem by running multiple Python interpreters in a single C++ process.
https://github.com/pytorch/torchsnapshot # A performant, memory-efficient checkpointing library for PyTorch applications, designed with large, complex distributed workloads in mind.
https://github.com/pytorch/rfcs # PyTorch RFCs (experimental)
https://github.com/pytorch/torchdistx # Torch Distributed Experimental
https://github.com/pytorch/csprng # Cryptographically secure pseudorandom number generators for PyTorch
https://github.com/pytorch/pytorch_sphinx_theme # PyTorch Sphinx Theme
https://github.com/pytorch/test-infra # The testing infrastructure for the main PyTorch repo. For example, this repo hosts the logic to track disabled tests and slow tests.
https://github.com/pytorch/expecttest #
https://github.com/pytorch/torchcodec # PyTorch video decoding
https://github.com/pytorch/pytorch-integration-testing # Testing downstream libraries using pytorch release candidates
https://github.com/pytorch/docs # This repository is automatically generated to contain the website source for the PyTorch documentation at https//pytorch.org/docs.
