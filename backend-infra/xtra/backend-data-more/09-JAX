https://github.com/googlecloudplatform/ai-on-gke # AI on GKE is a collection of examples, best-practices, and prebuilt solutions to help build, deploy, and scale AI Platforms on Google Kubernetes Engine
https://github.com/googlecloudplatform/t5x-on-vertex-ai # This repository compiles prescriptive guidance and code samples demonstrating how to operationalize Google Research T5X framework on Google Cloud Vertex AI.

https://github.com/google/jax # Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more
https://github.com/google/flax # Flax is a neural network library for JAX that is designed for flexibility.
https://github.com/google/jax-cfd # Computational Fluid Dynamics in JAX
https://github.com/google/jaxopt # Hardware accelerated, batchable and differentiable optimizers in JAX.
https://github.com/google/maxtext # A simple, performant and scalable Jax LLM!
https://github.com/google/brax # Massively parallel rigidbody physics simulation on accelerator hardware.
https://github.com/google/fedjax # FedJAX is a JAX-based open source library for Federated Learning simulations that emphasizes ease-of-use in research.
https://github.com/google/trax # Trax â€” Deep Learning with Clear Code and Speed
https://github.com/google/jaxonnxruntime # A user-friendly tool chain that enables the seamless execution of ONNX models using JAX as the backend.
https://github.com/google/commonlooputils # CLU lets you write beautiful training loops in JAX.
https://github.com/google/orbax # Orbax provides common checkpointing and persistence utilities for JAX users
https://github.com/google/autobound # AutoBound automatically computes upper and lower bounds on functions.
https://github.com/google/tree-math # Mathematical operations for JAX pytrees
https://github.com/google/rax # Rax is a Learning-to-Rank library written in JAX.
https://github.com/google/codex # Data compression in JAX
https://github.com/google/jaxite #
https://github.com/google/paxml # Pax is a Jax-based machine learning framework for training large scale models. Pax allows for advanced and fully configurable experimentation and parallelization, and has demonstrated industry leading model flop utilization rates.
https://github.com/google/etils # Collection of eclectic utils for python.
https://github.com/google/neural-tangents # Fast and Easy Infinite Neural Networks in Python
https://github.com/google/jaxcam #
https://github.com/google/jetstream # JetStream is a throughput and memory optimized engine for LLM inference on XLA devices, starting with TPUs (and GPUs in future -- PRs welcome).

https://github.com/nvidia/transformerengine # A library for accelerating training and inference using 8-bit floating point (FP8) precision on Hopper and Ada GPUs
https://github.com/nvidia/jax-toolbox # JAX-Toolbox
https://github.com/nvidia/dali # A GPU-accelerated library for deep learning training and inference applications.
https://github.com/nvidia/warp # A Python framework for high performance GPU simulation and graphics
