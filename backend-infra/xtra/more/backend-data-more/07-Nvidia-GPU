https://github.com/NVIDIA/k8s-device-plugin # NVIDIA device plugin for Kubernetes
https://github.com/NVIDIA/gpu-operator # NVIDIA GPU Operator creates/configures/manages GPUs atop Kubernetes
https://github.com/NVIDIA/k8s-dra-driver # Dynamic Resource Allocation (DRA) for NVIDIA GPUs in Kubernetes
https://github.com/NVIDIA/ais-k8s # Kubernetes Operator, ansible playbooks, and production scripts for large-scale AIStore deployments on Kubernetes.
https://github.com/NVIDIA/nvidia-terraform-modules # Infrastructure as code for GPU accelerated managed Kubernetes clusters.
https://github.com/NVIDIA/vgpu-device-manager # NVIDIA vGPU Device Manager manages NVIDIA vGPU devices on top of Kubernetes
https://github.com/NVIDIA/knavigator # knavigator is a development, testing, and optimization toolkit for AI/ML scheduling systems at scale on Kubernetes.
https://github.com/NVIDIA/k8s-driver-manager # The NVIDIA Driver Manager is a Kubernetes component which assist in seamless upgrades of NVIDIA Driver on each node of the cluster.
https://github.com/NVIDIA/k8s-nim-operator # An Operator for deployment and maintenance of NVIDIA NIMs and NeMo microservices in a Kubernetes environment.
https://github.com/NVIDIA/k8s-cc-manager # The NVIDIA CC Manager is a Kubernetes component that will enable required CC mode on supported NVIDIA GPUs
https://github.com/NVIDIA/kubectl-nv # Kubectl NV plugin, a tool for managing NVIDIA objects on a kubernetes cluster.
https://github.com/NVIDIA/aws-kube-ci # This CI tool aims to create a virtual machine on AWS with a GPU enabled Kubernetes master/node.

https://github.com/NVIDIA/DeepLearningExamples # State-of-the-Art Deep Learning scripts organized by models - easy to train and deploy with reproducible accuracy and performance on enterprise-grade infrastructure.
https://github.com/NVIDIA/NeMo # A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)
https://github.com/NVIDIA/apex # A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch
https://github.com/NVIDIA/FasterTransformer # Transformer related optimization, including BERT, GPT
https://github.com/NVIDIA/FastPhotoStyle # Style transfer, deep learning, feature transform
https://github.com/NVIDIA/flownet2-pytorch # Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks
https://github.com/NVIDIA/tacotron2 # Tacotron 2 - PyTorch implementation with faster-than-realtime inference
https://github.com/NVIDIA/MinkowskiEngine # Minkowski Engine is an auto-diff neural network library for high-dimensional sparse tensors
https://github.com/NVIDIA/pix2pixHD # Synthesizing and manipulating 2048x1024 images with conditional GANs
https://github.com/NVIDIA/TransformerEngine # A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and in
ference. https://github.com/NVIDIA/DALI # A GPU-accelerated library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications.
https://github.com/NVIDIA/modulus # Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods
https://github.com/NVIDIA/partialconv # A New Padding Scheme: Partial Convolution based Padding
https://github.com/NVIDIA/clara-train-examples # Example notebooks demonstrating how to use Clara Train to build Medical Imaging Deep Learning models
https://github.com/NVIDIA/PyProf # A GPU performance profiling tool for PyTorch models
https://github.com/NVIDIA/Megatron-LM # Ongoing research training transformer models at scale
https://github.com/NVIDIA/CleanUNet # Official PyTorch Implementation of CleanUNet (ICASSP 2022)
https://github.com/NVIDIA/Fuser # A Fusion Code Generator for NVIDIA GPUs (commonly known as "nvFuser")
https://github.com/NVIDIA/semantic-segmentation # Nvidia Semantic Segmentation monorepo
https://github.com/NVIDIA/NeMo-text-processing # NeMo text processing for ASR and TTS
https://github.com/NVIDIA/vid2vid # Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.
https://github.com/NVIDIA/sentiment-discovery # Unsupervised Language Modeling at scale for robust sentiment classification
https://github.com/NVIDIA/BigVGAN # Official PyTorch implementation of BigVGAN (ICLR 2023)
https://github.com/NVIDIA/aistore # AIStore: scalable storage for AI applications
https://github.com/NVIDIA/retinanet-examples # Fast and accurate object detection with end-to-end GPU optimization
https://github.com/NVIDIA/waveglow # A Flow-based Generative Network for Speech Synthesis
https://github.com/NVIDIA/mellotron # Mellotron: a multispeaker voice synthesis model based on Tacotron 2 GST that can make a voice emote and sing without emotive or singing training data
https://github.com/NVIDIA/nvvl # A library that uses hardware acceleration to load sequences of video frames to facilitate machine learning training
https://github.com/NVIDIA/TorchFort # An Online Deep Learning Interface for HPC programs on NVIDIA GPUs
https://github.com/NVIDIA/tao_pytorch_backend # TAO Toolkit deep learning networks with PyTorch backend
https://github.com/NVIDIA/NVFlare # NVIDIA Federated Learning Application Runtime Environment
https://github.com/NVIDIA/torch-harmonics # Differentiable signal processing on the sphere for PyTorch
https://github.com/NVIDIA/transformer-ls # Official PyTorch Implementation of Long-Short Transformer (NeurIPS 2021).
https://github.com/NVIDIA/pyxis # Container plugin for Slurm Workload Manager
https://github.com/NVIDIA/modulus-makani # Massively parallel training of machine-learning based weather and climate models
https://github.com/NVIDIA/TensorRT-LLM # TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.
https://github.com/NVIDIA/ContrastiveLosses4VRD # Implementation for the CVPR2019 paper "Graphical Contrastive Losses for Scene Graph Generation"
https://github.com/NVIDIA/flowtron # Flowtron is an auto-regressive flow-based generative network for text to speech synthesis with control over speech variation and style transfer
https://github.com/NVIDIA/audio-flamingo # PyTorch implementation of Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities.
https://github.com/NVIDIA/modulus-launch # Repo of optimized training recipes for accelerating PyTorch workflows of AI driven surrogates for physical systems
https://github.com/NVIDIA/TensorRT-Model-Optimizer # TensorRT Model Optimizer is a unified library of state-of-the-art model optimization techniques such as quantization, sparsity, distillation, etc. It compresses deep learning models for downstream deployment frameworks like TensorRT-LLM or TensorRT to optimize inference speed on NVIDIA GPUs.
https://github.com/NVIDIA/GraphQSat # Using GNN and DQN to find a baetter branching heuristic for a CDCL Solver
https://github.com/NVIDIA/ngc-container-replicator # NGC Container Replicator
https://github.com/NVIDIA/LDDL # Distributed preprocessing and data loading for language datasets
https://github.com/NVIDIA/healthcare-on-tap-TRT-TRITON-demo # Demonstration of the use of TensorRT and TRITON
https://github.com/NVIDIA/modulus-sym # Framework providing pythonic APIs, algorithms and utilities to be used with Modulus core to physics inform model training as well as higher level abstraction for domain experts
https://github.com/NVIDIA/warp # A Python framework for high performance GPU simulation and graphics
https://github.com/NVIDIA/UnsupervisedLandmarkLearning # Implementation for the unsupervised latent landmark learning work from NVIDIA Applied Deep Learning Research
https://github.com/NVIDIA/mlperf-common # NVIDIA's launch, startup, and logging scripts used by our MLPerf Training and HPC submissions
https://github.com/NVIDIA/ngc-container-environment-modules # Environment modules for NGC containers
https://github.com/NVIDIA/tao_tutorials # Quick start scripts and tutorial notebooks to get started with TAO Toolkit
https://github.com/NVIDIA/dlinput-tf # Optimized data input pipeline for deep learning frameworks
https://github.com/NVIDIA/MegaMolBART # A deep learning model for small molecule drug discovery and cheminformatics based on SMILES
https://github.com/NVIDIA/free-threaded-python # No-GIL Python environment featuring NVIDIA Deep Learning libraries.
https://github.com/NVIDIA/nvImageCodec # A nvImageCodec library of GPU- and CPU- accelerated codecs featuring a unified interface
https://github.com/NVIDIA/RAD-MMM # A TTS model that makes a speaker speak new languages
https://github.com/NVIDIA/runx # Deep Learning Experiment Management
https://github.com/NVIDIA/cccl # CUDA Core Compute Libraries
https://github.com/NVIDIA/tao_launcher # Lightweight Python based CLI application to run TAO Toolkit
https://github.com/NVIDIA/gpu_affinity # GPU Affinity is a package to automatically set the CPU process affinity to match the hardware architecture on a given platform

https://github.com/NVIDIA/tensorflow # An Open Source Machine Learning Framework for Everyone
https://github.com/NVIDIA/DeepLearningExamples # State-of-the-Art Deep Learning scripts organized by models - easy to train and deploy with reproducible accuracy and performance on enterprise-grade infrastructure.
https://github.com/NVIDIA/FasterTransformer # Transformer related optimization, including BERT, GPT
https://github.com/NVIDIA/DIGITS # Deep Learning GPU Training System
https://github.com/NVIDIA/DALI # A GPU-accelerated library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications.
https://github.com/NVIDIA/clara-train-examples # Example notebooks demonstrating how to use Clara Train to build Medical Imaging Deep Learning models
https://github.com/NVIDIA/fsi-samples # A collection of open-source GPU accelerated Python tools and examples for quantitative analyst tasks and leverages RAPIDS AI project, Numba, cuDF, and Dask.
https://github.com/NVIDIA/framework-reproducibility # Providing reproducibility in deep learning frameworks
https://github.com/NVIDIA/NVFlare # NVIDIA Federated Learning Application Runtime Environment
https://github.com/NVIDIA/sampleQAT # Inference of quantization aware trained networks using TensorRT
https://github.com/NVIDIA/pix2pixHD # Synthesizing and manipulating 2048x1024 images with conditional GANs
https://github.com/NVIDIA/atex # A TensorFlow Extension: GPU performance tools for TensorFlow.
https://github.com/NVIDIA/ngc-container-replicator # NGC Container Replicator
https://github.com/NVIDIA/vdisc # VDisc is a tool for creating and mounting virtual CD-ROM images backed by object storage
https://github.com/NVIDIA/Imageinary # Imageinary is a reproducible mechanism which is used to generate large image datasets at various resolutions. The tool supports multiple image types, including JPEGs, PNGs, BMPs, RecordIO, and TFRecord files
https://github.com/NVIDIA/TensorRT # NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT.
https://github.com/NVIDIA/tao_tensorflow1_backend # TAO Toolkit deep learning networks with TensorFlow 1.x backend
https://github.com/NVIDIA/tao_tensorflow2_backend # TAO Toolkit deep learning networks with TensorFlow 2.x backend
https://github.com/NVIDIA/dlinput-tf # Optimized data input pipeline for deep learning frameworks
https://github.com/NVIDIA/vid2vid # Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.
https://github.com/NVIDIA/mlperf-common # NVIDIA's launch, startup, and logging scripts used by our MLPerf Training and HPC submissions
https://github.com/NVIDIA/tao_tutorials # Quick start scripts and tutorial notebooks to get started with TAO Toolkit
https://github.com/NVIDIA/go-tfdata # Go library that provides easy-to-use interfaces and tools for TensorFlow users, in particular allowing to train existing TF models on .tar and .tgz datasets
https://github.com/NVIDIA/cccl # CUDA Core Compute Libraries
https://github.com/NVIDIA/tao_launcher # Lightweight Python based CLI application to run TAO Toolkit
https://github.com/NVIDIA/Bobber # Containerized testing of system components that impact AI workload performance
